{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaH9MzWcBZiK"
   },
   "source": [
    "### 1. Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zz3cz4i1BZiN",
    "outputId": "056efec1-2ab4-4fc3-fa86-43cc9b63d949"
   },
   "outputs": [],
   "source": [
    "import re, numpy as np, pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PO1CfVe0BZiY"
   },
   "source": [
    "### Tasks in Class\n",
    "\n",
    "Suppose a vandal has broken into your study and torn apart four of your books:\n",
    "\n",
    "* *Great Expectations* by Charles Dickens\n",
    "* *The War of the Worlds* by H.G. Wells\n",
    "* *Twenty Thousand Leagues Under the Sea* by Jules Verne\n",
    "* *Pride and Prejudice* by Jane Austen\n",
    "\n",
    "This vandal has torn the books into individual chapters, and left them in one large pile. How can we restore these disorganized chapters to their original books? This is a challenging problem since the individual chapters are unlabeled: we don’t know what words might distinguish them into groups. We’ll thus use topic modeling to discover how chapters cluster into distinct topics, each of them (presumably) representing one of the books.\n",
    "The code below loads the chapters of the four books and returns a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "vdR3uX0kBZiY",
    "outputId": "76de6738-33de-4554-aa1c-3593832c9e69"
   },
   "outputs": [],
   "source": [
    "%run \"CorpusUtils.py\"\n",
    "#books = makeCleanCorpus(abspath = os.path.abspath('.') + '/data/LibraryHeist/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "ABogzKLMBZiY"
   },
   "outputs": [],
   "source": [
    "#import pickle\n",
    "#pickle.dump( books, open( \"data/books.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "-XomyClZBZiY"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "books = pickle.load( open( \"data/books.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "ijjuopJrBZiY",
    "outputId": "92b8e255-e210-4855-d2b4-7fb6d549b148",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "id": "nlY0uZ0gBZiZ"
   },
   "outputs": [],
   "source": [
    "#change dictionary to list: \n",
    "#(help: how do I keep the names of the chapters ???)\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "books_words = list(sent_to_words(books.values()))\n",
    "\n",
    "#print(books_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "CLBpyICqBZiZ"
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(books_words)\n",
    "\n",
    "# Create Corpus: Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in books_words]\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=4,                                            \n",
    "                                           per_word_topics=True,\n",
    "                                           random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel, corpus, texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row_list in enumerate(ldamodel[corpus]):\n",
    "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
    "        # print(row)\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8372</td>\n",
       "      <td>elizabeth, miss, like, us, went, saw, came, jo...</td>\n",
       "      <td>[chapter, fathers, family, name, pirrip, chris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9972</td>\n",
       "      <td>joe, us, captain, thought, nautilus, saw, like...</td>\n",
       "      <td>[chapter, ii, sister, joe, gargery, twenty, ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.5179</td>\n",
       "      <td>joe, us, captain, thought, nautilus, saw, like...</td>\n",
       "      <td>[chapter, iii, rimy, morning, damp, seen, damp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9993</td>\n",
       "      <td>joe, us, captain, thought, nautilus, saw, like...</td>\n",
       "      <td>[chapter, iv, fully, expected, find, constable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.8326</td>\n",
       "      <td>joe, us, captain, thought, nautilus, saw, like...</td>\n",
       "      <td>[chapter, apparition, file, soldiers, ringing,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.9702</td>\n",
       "      <td>joe, us, captain, thought, nautilus, saw, like...</td>\n",
       "      <td>[chapter, felicitous, idea, occurred, morning,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.8968</td>\n",
       "      <td>elizabeth, miss, like, us, went, saw, came, jo...</td>\n",
       "      <td>[chapter, xi, appointed, returned, miss, havis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7476</td>\n",
       "      <td>miss, us, like, elizabeth, went, joe, came, sa...</td>\n",
       "      <td>[chapter, xii, grew, uneasy, subject, pale, yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.6796</td>\n",
       "      <td>miss, us, like, elizabeth, went, joe, came, sa...</td>\n",
       "      <td>[chapter, xiii, trial, feelings, next, joe, ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9971</td>\n",
       "      <td>elizabeth, miss, like, us, went, saw, came, jo...</td>\n",
       "      <td>[chapter, xiv, miserable, thing, feel, ashamed...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0             1.0              0.8372   \n",
       "1            1             2.0              0.9972   \n",
       "2            2             2.0              0.5179   \n",
       "3            3             2.0              0.9993   \n",
       "4            4             2.0              0.8326   \n",
       "5            5             2.0              0.9702   \n",
       "6            6             1.0              0.8968   \n",
       "7            7             0.0              0.7476   \n",
       "8            8             0.0              0.6796   \n",
       "9            9             1.0              0.9971   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  elizabeth, miss, like, us, went, saw, came, jo...   \n",
       "1  joe, us, captain, thought, nautilus, saw, like...   \n",
       "2  joe, us, captain, thought, nautilus, saw, like...   \n",
       "3  joe, us, captain, thought, nautilus, saw, like...   \n",
       "4  joe, us, captain, thought, nautilus, saw, like...   \n",
       "5  joe, us, captain, thought, nautilus, saw, like...   \n",
       "6  elizabeth, miss, like, us, went, saw, came, jo...   \n",
       "7  miss, us, like, elizabeth, went, joe, came, sa...   \n",
       "8  miss, us, like, elizabeth, went, joe, came, sa...   \n",
       "9  elizabeth, miss, like, us, went, saw, came, jo...   \n",
       "\n",
       "                                                Text  \n",
       "0  [chapter, fathers, family, name, pirrip, chris...  \n",
       "1  [chapter, ii, sister, joe, gargery, twenty, ye...  \n",
       "2  [chapter, iii, rimy, morning, damp, seen, damp...  \n",
       "3  [chapter, iv, fully, expected, find, constable...  \n",
       "4  [chapter, apparition, file, soldiers, ringing,...  \n",
       "5  [chapter, felicitous, idea, occurred, morning,...  \n",
       "6  [chapter, xi, appointed, returned, miss, havis...  \n",
       "7  [chapter, xii, grew, uneasy, subject, pale, yo...  \n",
       "8  [chapter, xiii, trial, feelings, next, joe, ar...  \n",
       "9  [chapter, xiv, miserable, thing, feel, ashamed...  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=books_words)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column with true labels\n",
    "df_dominant_topic[\"True_Chapter\"] = books.keys()\n",
    "df_dominant_topic[\"True_Chapter\"] = df_dominant_topic.True_Chapter.str.replace(\"x|.txt\", \"\", regex=True) # remove .txt and x in some GE\n",
    "\n",
    "# Normalize naming for books\n",
    "df_dominant_topic[\"True_Book\"] = df_dominant_topic.True_Chapter.str.extract(\"^(\\w*?)(?=[r]?_)\")\n",
    "df_dominant_topic[\"True_Book\"] = df_dominant_topic.True_Book.str.replace(\"w\", \"\", regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GE', 'PaP', 'TTLutS', 'TWotW'], dtype=object)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check different names\n",
    "df_dominant_topic.True_Book.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>True_Book</th>\n",
       "      <th>GE</th>\n",
       "      <th>PaP</th>\n",
       "      <th>TTLutS</th>\n",
       "      <th>TWotW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>28</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "True_Book       GE  PaP  TTLutS  TWotW\n",
       "Dominant_Topic                        \n",
       "0.0             16    1       1     20\n",
       "1.0             28   59       2      3\n",
       "2.0             13    1      14      3\n",
       "3.0              2    0      29      1"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check how many chapters from different books have been clustered to each topic\n",
    "pd.crosstab(df_dominant_topic.Dominant_Topic, df_dominant_topic.True_Book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We were not able to restore the disorganized chapters to their original books\n",
    "- However, almost all chapters from `TTLutS` and `PaP` were assigned to only one cluster \n",
    "- Especially the chapters from `GE` and `TTLutS` are distributed across multiple clusters"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "TopicModels_LDA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
